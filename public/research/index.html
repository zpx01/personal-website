<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zeeshan Patel</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zeeshan Patel</name>
              </p>
              <p>I am an EECS M.S. student at UC Berkeley. I graduated from UC Berkeley with my bachelor's in CS + Stats in May 2024. I am advised by <a href="https://people.eecs.berkeley.edu/~efros/">Professor Alexei Efros</a> and <a href="https://people.eecs.berkeley.edu/~malik/">Professor Jitendra Malik</a> at <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR)</a>.
              </p>
              <p>
                Currently, I am interning at <a href="https://www.nvidia.com/en-us/research/">NVIDIA</a> working on large scale vision models. Previously, I've interned at <a href="https://machinelearning.apple.com/">Apple AI/ML</a> within the Information Intelligence team, focusing on foundation models, and at <a href="https://www.verkada.com/">Verkada</a> on the Special Projects team.
              </p>
              <p>
                Feel free to reach out to me through email or LinkedIn to chat about research or any potential collaborations.
              </p>
              <p style="text-align:center">
                <a href="mailto:zeeshanp@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zeeshan-patel">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/zpx01/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=FTbkVd8AAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zeeshan.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zeeshan.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in computer vision and deep learning. Specifically, I'm interested in scaling generative models in the visual domain and developing methods that efficiently utilize compute / data for deep learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;table-layout:fixed;"><tbody>
          
          <tr>
            <td style="padding:15px;width:50%;vertical-align:top;">
                <div class="one" style="width:100%;text-align:center;">
                <img src='images/scaling_diffusion.png' style="display:block; max-width:365px;margin-left:auto;margin-right:auto;margin-top:50px;">
                </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
                <a href="#">
                <papertitle>Scaling Properties of Diffusion Models For Perceptual Tasks</papertitle>
                </a>
                <br>
                <strong>Zeeshan Patel*</strong>,
                <a href="#">Rahul Ravishankar*</a>, 
                <a href="https://brjathu.github.io">Jathushan Rajasegaran</a>,
                <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>Under Review</em>, 2024
                <br>
                <a href="https://scaling-diffusion-perception.github.io/">project page</a> / <a href="#">arXiv [coming soon]</a> / <a href="">code [coming soon]</a>
                <br>
                <p>
                We argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks.
                </p>
            </td>
            </tr>
          <tr>
          <tr>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <div class="one" style="width:100%;text-align:center;">
                <img src='images/swag_inference.png' style="display:block; max-width:175px;margin-left:auto;margin-right:auto;">
              </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <a href="data/swag.pdf">
                <papertitle>SWAG: Storytelling With Action Guidance</papertitle>
              </a>
              <br>
              <strong>Zeeshan Patel*</strong>,
              <a href="https://jonnypei.github.io/">Jonathan Pei*</a>, 
              <a href="https://www.linkedin.com/in/karim-el-refai/">Karim El-Refai*</a>,
              <a href="https://www.linkedin.com/in/tianleli/">Tianle Li</a>
              <br>
              <em>EMNLP Findings</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2402.03483">arXiv</a> / <a href="">code [coming soon]</a>
              <br>
              <p>
                We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop. SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5- Turbo.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <div class="one" style="width:100%;text-align:center;">
                <img src='images/ttt_diagram_copy.png' style="display:block; max-width:350px;margin-left:auto;margin-right:auto;">
              </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <a href="data/ttt_sr.pdf">
                <papertitle>Test-Time Training for Image Superresolution</papertitle>
              </a>
              <br>
              <strong>Zeeshan Patel*</strong>,
              <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>
              <br>
              <em>Preprint</em>, 2023
              <br>
              <p>
                A self-supervised approach for fine-tuning image superresolution models to adapt to new test distributions on-the-fly.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table>
          <tr>
              <div style="width:100%;text-align:center;">
                <p>Template by <a href="https://jonbarron.info/">Jon Barron</a>.</p>
              </div>
          </tr>
        </table>        
      </td>
    </tr>
  </table>
</body>

</html>
