<!DOCTYPE HTML>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zeeshan Patel</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <style>
    /* For screens below 768px wide (adjust as needed) */
    @media screen and (max-width: 768px) {
      /* Make each row display as a block rather than a table row */
      .research-project-row {
        display: block;
        margin-bottom: 20px; /* optional: adds space between rows */
      }
      /* Force each table cell to take the full width in these rows */
      .research-project-row td {
        display: block;
        width: 100% !important;
        margin: 10px 0;
      }
      /* Optionally center images when stacked */
      .research-project-row img {
        margin-left: auto;
        margin-right: auto;
      }

    }

    @media screen and (max-width: 768px) {
      .intro-row {
        /* For smaller screens, treat this row as a block 
          so the cells stack vertically */
        display: block;
        margin-bottom: 20px; /* optional spacing */
      }
      .intro-row td {
        display: block;
        width: 100% !important;
        margin: 10px 0;
      }
      /* Center the image (optional) */
      .intro-row img {
        margin-left: auto;
        margin-right: auto;
        display: block;
      }
    }
  </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BB3K7090ET"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BB3K7090ET');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px" class="research-project-row">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zeeshan Patel</name>
              </p>
              <p>I am an EECS M.S. student at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~efros/" target="_blank">Professor Alexei Efros</a> and <a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Professor Jitendra Malik</a> at <a href="https://bair.berkeley.edu/" target="_blank">Berkeley Artificial Intelligence Research (BAIR)</a>. I graduated with honors from UC Berkeley with a Bachelor's in CS + Statistics in May 2024.
              </p>
              <p>
                Currently, I work on <a href="https://www.nvidia.com/en-us/ai/cosmos/">generative world models</a> at <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research</a>. 
                Previously, I worked on foundation models at <a href="https://machinelearning.apple.com/" target="_blank">Apple AI/ML</a>.
              </p>
              <p>
                Feel free to reach out to chat about research / potential collaborations.
              </p>
              <p style="text-align:center">
                <a href="mailto:zeeshanp@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://x.com/zeeshanp_" target="_blank">X</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zeeshan-patel" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/zpx01/" target="_blank">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=FTbkVd8AAAAJ&hl=en" target="_blank">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zeeshan.JPG">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/zeeshan.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in deep learning, generative models, and physical AI. Specifically, I'm interested in scaling deep learning with principled techniques that efficiently utilize data and compute.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;table-layout:fixed;"><tbody>
          <tr class="research-project-row">
            <td style="display:flex;vertical-align:top;">
                <div class="one" style="width:100%;text-align:center;margin-bottom: 50px;">
                <img src='images/cosmos.jpg' style="display:block;max-width:300px;margin-left:auto;margin-right:auto;">
                </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;padding-bottom: 30px;">
                <a href="https://arxiv.org/abs/2501.03575" target="_blank">
                <papertitle>Cosmos World Foundation Model Platform for Physical AI</papertitle>
                </a>
                <br>
                NVIDIA: <strong>Zeeshan Patel</strong> (Contributor),
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://research.nvidia.com/labs/dir/cosmos1/" target="_blank">project page</a> / <a href="https://arxiv.org/abs/2501.03575" target="_blank">arXiv</a> / <a href="https://github.com/nvidia/cosmos" target="_blank">code</a> / <a href="https://www.youtube.com/live/k82RwXqZHY8?t=3303s" target="_blank">keynote</a> / press: <a href="https://www.nytimes.com/2025/01/07/business/dealbook/nvidia-ai-robots.html" target="_blank">New York Times</a>, <a href="https://www.wsj.com/tech/ai/nvidia-ceo-pitches-robotics-cars-as-growth-areas-to-consumer-electronics-audience-68905f2d" target="_blank">Wall Street Journal</a>, <a href="https://fortune.com/2025/01/06/nvidia-new-ai-platform-robotics-chatgpt-moment-robots-self-driving-cars/" target="_blank">Fortune</a>, <a href="https://techcrunch.com/2025/01/06/nvidia-releases-its-own-brand-of-world-models/" target="_blank">TechCrunch</a>, <a href="https://www.forbes.com/sites/johnwerner/2025/01/19/understanding-the-physics-aware-systems-that-nvidia-is-working-on/" target="_blank">Forbes</a>, <a href="https://www.wired.com/story/nvidia-cosmos-ai-helps-robots-self-driving-cars/" target="_blank">Wired</a>, <a href="https://www.bbc.com/news/articles/c0q0jl8pl9ko" target="_blank">BBC</a>
                <p>
                Generative world foundation models for data-driven simulation of physical AI systems.
                </p>
            </td>
            </tr>
          <tr>
            <tr class="research-project-row">
              <td style="display:flex;padding-left:30px;width:50%;vertical-align:top;">
                  <div class="one" style="width:100%;text-align:center;margin-bottom: 35px;">
                  <img src='images/nemo_vfm.png' style="display:block;max-width:300px;margin-left:auto;margin-right:auto">
                  </div>
              </td>
              <td style="padding:15px;width:50%;vertical-align:top;">
                  <a href="https://arxiv.org/abs/2503.12964" target="_blank">
                  <papertitle>Training Video Foundation Models with NVIDIA NeMo</papertitle>
                  </a>
                  <br>
                  NVIDIA: <strong>Zeeshan Patel</strong> (Lead Contributor),
                  <br>
                  <a href="https://arxiv.org/abs/2503.12964" target="_blank">arXiv</a> / <a href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/" target="_blank">NVIDIA technical blog</a> / <a href="https://github.com/NVIDIA/NeMo/tree/main/nemo/collections/diffusion" target="_blank">code</a>
                  <br>
                  <p>
                    Open-source video foundation model training framework, providing accelerated video dataset curation,
                    multimodal dataloading, and parallelized video diffusion model training and inference.
                  </p>
              </td>
              </tr>
            <tr>
          <tr class="research-project-row">
            <td style="display:flex;padding:15px;width:50%;vertical-align:top;">
                <div class="one" style="width:100%;text-align:center;">
                <img src='images/scaling_diffusion.png' style="display:block;max-width:350px;margin-left:auto;margin-right:auto;margin-top:50px;">
                </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
                <a href="#">
                <papertitle>Scaling Properties of Diffusion Models For Perceptual Tasks</papertitle>
                </a>
                <br>
                <strong>Zeeshan Patel*</strong>,
                <a href="https://rravishankar1.github.io/" target="_blank">Rahul Ravishankar*</a>, 
                <a href="https://brjathu.github.io" target="_blank">Jathushan Rajasegaran</a>,
                <a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a>
                <br>
                <em>CVPR 2025</em>
                <br>
                <a href="https://scaling-diffusion-perception.github.io/" target="_blank">project page</a> / <a href="https://arxiv.org/abs/2411.08034" target="_blank">arXiv</a> / <a href="https://github.com/scaling-diffusion-perception/scaling-diffusion-perception" target="_blank">code</a>
                <br>
                <p>
                Iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks.
                </p>
            </td>
            </tr>
          <tr class="research-project-row">
            <td style="display:flex;padding:15px;width:50%;vertical-align:top;">
              <div class="one" style="width:100%;">
                <img src='images/swag_inference.png' style="display:block;margin:auto;max-width: 170px;padding-left:80px;">
              </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <a href="data/swag.pdf" target="_blank">
                <papertitle>SWAG: Storytelling With Action Guidance</papertitle>
              </a>
              <br>
              <strong>Zeeshan Patel*</strong>,
              <a href="https://jonnypei.github.io/" target="_blank">Jonathan Pei*</a>, 
              <a href="https://www.linkedin.com/in/karim-el-refai/" target="_blank">Karim El-Refai*</a>,
              <a href="https://www.linkedin.com/in/tianleli/" target="_blank">Tianle Li</a>
              <br>
              <em>EMNLP</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2402.03483">arXiv</a>
              <br>
              <p>
                We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop. SWAG can optimize open-sourced LLMs to substantially outperform previous end-to-end story generation techniques leveraging closed-source models.
              </p>
            </td>
          </tr>
          <tr>
            <tr class="research-project-row">
              <td style="display:flex;padding:15px;width:50%;vertical-align:top;">
                  <div class="one" style="width:100%;text-align:center;">
                  <img src='images/exploringmatching.png' style="display:block;max-height:200px;max-width:150px;margin-left:100px;margin-right:auto;margin-top:25px;">
                  </div>
              </td>
              <td style="padding:15px;width:50%;vertical-align:top;">
                  <a href="data/exploring_matching.pdf" target="_blank">
                  <papertitle>Exploring Diffusion and Flow Matching Under Generator Matching</papertitle>
                  </a>
                  <br>
                  <strong>Zeeshan Patel*</strong>,
                  <a href="https://jamesdeloye.dev">James DeLoye</a>,
                  <a href="#">Lance Mathias</a>
                  <br>
                  <em>Preprint</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2412.11024" target="_blank">arXiv</a>
                  <br>
                  <p>
                  We explore diffusion and flow matching models under the theoretical framework of generator matching. Our analysis offers a fresh perspective on the relationships between these state-of-the-art generative modeling paradigms and how to build new generative Markov processes that benefit from both approaches. 
                  </p>
              </td>
          </tr>
          <tr class="research-project-row">
            <td style="padding:15px;width:50%;vertical-align:top;">
              <div class="one" style="width:100%;text-align:center;">
                <img src='images/ttt_diagram_copy.png' style="display:block;max-width:350px;margin-left:auto;margin-right:auto;">
              </div>
            </td>
            <td style="padding:15px;width:50%;vertical-align:top;">
              <a href="data/ttt_sr.pdf">
                <papertitle>Test-Time Training for Image Superresolution</papertitle>
              </a>
              <br>
              <strong>Zeeshan Patel*</strong>,
              <a href="https://yossigandelsman.github.io/" target="_blank">Yossi Gandelsman</a>
              <br>
              <em>Preprint</em>, 2023
              <br>
              <a href="data/ttt_sr.pdf" target="_blank">paper</a> / <a href="https://github.com/zpx01/image-sr" target="_blank">code</a>
              <br>
              <p>
                We present a self-supervised test-time training approach for fine-tuning image superresolution models to adapt to new test distributions on-the-fly.
              </p>
            </td>
          </tr>
        </tbody></table>
        <br>
        <br>
        <table> 
          <tr>
              <div style="width:100%;text-align:center;">
                <p>Template by <a href="https://jonbarron.info/">Jon Barron</a>.</p>
              </div>
          </tr>
        </table>        
      </td>
    </tr>
  </table>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
</body>

</html>
